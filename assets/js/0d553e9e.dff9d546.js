"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[3532],{3118:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>a});var o=n(5893),c=n(1151);const t={sidebar_position:1},i="CEPH Troubleshooting",r={id:"ceph/troubleshooting-ceph",title:"CEPH Troubleshooting",description:"This is not official documentation for AutomationSuite",source:"@site/docs/ceph/troubleshooting-ceph.md",sourceDirName:"ceph",slug:"/ceph/troubleshooting-ceph",permalink:"/automation-suite-support-tools/docs/ceph/troubleshooting-ceph",draft:!1,unlisted:!1,editUrl:"https://uipath.github.com/UiPath/automation-suite-support-tools/edit/master/docs/docs/ceph/troubleshooting-ceph.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Ceph Troubleshooting",permalink:"/automation-suite-support-tools/docs/category/ceph-troubleshooting"},next:{title:"CEPH Rebuilding OSD",permalink:"/automation-suite-support-tools/docs/ceph/ceph-rebuilding-osd"}},l={},a=[{value:"Identify if CEPH is unhealthy",id:"identify-if-ceph-is-unhealthy",level:2},{value:"Tools required for debugging",id:"tools-required-for-debugging",level:2},{value:"Query CEPH via s3cmd",id:"query-ceph-via-s3cmd",level:2},{value:"Query Size of the Buckets",id:"query-size-of-the-buckets",level:2},{value:"Query size of buckets using s3cmd",id:"query-size-of-buckets-using-s3cmd",level:2},{value:"Query No of Objects Pending garbage collection",id:"query-no-of-objects-pending-garbage-collection",level:2},{value:"Troubleshooting ceph",id:"troubleshooting-ceph",level:2},{value:"OSD(s) are crashing due to Init error",id:"osds-are-crashing-due-to-init-error",level:2},{value:"PGs stuck in Incomplete state",id:"pgs-stuck-in-incomplete-state",level:2},{value:"PGs stuck in recovery_unfound state",id:"pgs-stuck-in-recovery_unfound-state",level:2},{value:"Clock skew in ceph cluster",id:"clock-skew-in-ceph-cluster",level:2}];function d(e){const s={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,c.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(s.h1,{id:"ceph-troubleshooting",children:"CEPH Troubleshooting"}),"\n",(0,o.jsx)(s.admonition,{title:"Note",type:"danger",children:(0,o.jsx)(s.p,{children:"This is not official documentation for AutomationSuite"})}),"\n",(0,o.jsx)(s.h2,{id:"identify-if-ceph-is-unhealthy",children:"Identify if CEPH is unhealthy"}),"\n",(0,o.jsx)(s.p,{children:"If ceph objectstore is not consumable , the dependent application will become unhealthy. You can run below command to identify Unhealthy Pods in the system"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl get pod -A -o wide | grep  --color -P '([0-9])/\\1|Completed'  -v\n"})}),"\n",(0,o.jsx)(s.h2,{id:"tools-required-for-debugging",children:"Tools required for debugging"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"rook-ceph-tools"}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"s3cmd (one can use sf-k8-utils-rhel to spawn a temp pod for s3cmd)"}),"\n",(0,o.jsxs)(s.ol,{children:["\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Run these commands on any server node"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:'OBJECT_GATEWAY_INTERNAL_IP="rook-ceph-rgw-rook-ceph"\nOBJECT_GATEWAY_INTERNAL_HOST=$(kubectl -n rook-ceph get services/$OBJECT_GATEWAY_INTERNAL_IP -o jsonpath="{.spec.clusterIP}")\nOBJECT_GATEWAY_INTERNAL_PORT=$(kubectl -n rook-ceph get services/$OBJECT_GATEWAY_INTERNAL_IP -o jsonpath="{.spec.ports[0].port}")\nSTORAGE_ACCESS_KEY=$(kubectl -n rook-ceph get secret ceph-object-store-secret -o json | jq \'.data.OBJECT_STORAGE_ACCESSKEY\' | sed -e \'s/^"//\' -e \'s/"$//\' | base64 -d)\nSTORAGE_SECRET_KEY=$(kubectl -n rook-ceph get secret ceph-object-store-secret -o json | jq \'.data.OBJECT_STORAGE_SECRETKEY\' | sed -e \'s/^"//\' -e \'s/"$//\' | base64 -d)\necho "export AWS_HOST=$OBJECT_GATEWAY_INTERNAL_HOST"\necho "export AWS_ENDPOINT=$OBJECT_GATEWAY_INTERNAL_HOST:$OBJECT_GATEWAY_INTERNAL_PORT"\necho "export AWS_ACCESS_KEY_ID=$STORAGE_ACCESS_KEY"\necho "export AWS_SECRET_ACCESS_KEY=$STORAGE_SECRET_KEY"\n'})}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Spin a pod with s3cmd installed"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:'sf_utils=$(kubectl get pods -n uipath-infra -o jsonpath="{.items[*].spec.containers[*].image}" | tr -s \'[[:space:]]\' \'\\n\' | sort | uniq | grep sf-k8-utils | head -1)\n[[ -z "${sf_utils}" ]] && echo "Unable to get sf_utils image , Please set the image manually \'sf_utils=<IMAGE_NAME>\'"\n\n  if [[ -n "${sf_utils}" ]]; then\n  cat > /tmp/dummy-pod-s3cmd.yaml << \'EOF\'\n  apiVersion: v1\n  kind: Pod\n  metadata:\n  name: s3cmd\n  namespace: uipath-infra\n  spec:\n  containers:\n  - image: __IMAGE_PLACEHOLDER__\n      imagePullPolicy: IfNotPresent\n      command: ["/bin/bash"]\n      args: ["-c", "tail -f /dev/null"]\n      name: rclone\n      resources: {}\n      terminationMessagePath: /dev/termination-log\n      terminationMessagePolicy: File\n      securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n          drop:\n          - ALL\n          - NET_RAW\n      privileged: false\n      readOnlyRootFilesystem: true\n      runAsGroup: 1\n      runAsNonRoot: true\n      runAsUser: 1\n      volumeMounts:\n      - mountPath: /.kube\n      name: kubedir\n  volumes:\n  - emptyDir: {}\n      name: kubedir      \n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  terminationGracePeriodSeconds: 30\n  EOF\n  sed -i "s;__IMAGE_PLACEHOLDER__;${sf_utils};g" /tmp/dummy-pod-s3cmd.yaml\n  kubectl apply -f  /tmp/dummy-pod-s3cmd.yaml\n  fi\n'})}),"\n",(0,o.jsxs)(s.ol,{start:"3",children:["\n",(0,o.jsx)(s.li,{children:"Login to the s3cmd pod"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"   kubectl -n uipath-infra exec -it s3cmd -- bash\n"})}),"\n",(0,o.jsxs)(s.ol,{start:"4",children:["\n",(0,o.jsx)(s.li,{children:"copy/paste output of first step into the s3cmd pod console, output of first command will look similar to this"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"export AWS_HOST=10.43.89.191\nexport AWS_ENDPOINT=10.43.89.191:80\nexport AWS_ACCESS_KEY_ID=xxxx\nexport AWS_SECRET_ACCESS_KEY=xxxx\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.h2,{id:"query-ceph-via-s3cmd",children:"Query CEPH via s3cmd"}),"\n",(0,o.jsx)(s.p,{children:"Install s3cmd client,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"sudo pip3 install s3cmd\n"})}),"\n",(0,o.jsx)(s.p,{children:"To access ceph internally,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"OBJECT_GATEWAY_INTERNAL_IP=\"rook-ceph-rgw-rook-ceph\"\nOBJECT_GATEWAY_INTERNAL_HOST=$(kubectl -n rook-ceph get services/$OBJECT_GATEWAY_INTERNAL_IP -o jsonpath=\"{.spec.clusterIP}\")\nOBJECT_GATEWAY_INTERNAL_PORT=$(kubectl -n rook-ceph get services/$OBJECT_GATEWAY_INTERNAL_IP -o jsonpath=\"{.spec.ports[0].port}\")\nSTORAGE_ACCESS_KEY=$(kubectl -n rook-ceph get secret ceph-object-store-secret -o json | jq '.data.OBJECT_STORAGE_ACCESSKEY' | sed -e 's/^\"//' -e 's/\"$//' | base64 -d)\nSTORAGE_SECRET_KEY=$(kubectl -n rook-ceph get secret ceph-object-store-secret -o json | jq '.data.OBJECT_STORAGE_SECRETKEY' | sed -e 's/^\"//' -e 's/\"$//' | base64 -d)\nexport AWS_HOST=$OBJECT_GATEWAY_INTERNAL_HOST\nexport AWS_ENDPOINT=$OBJECT_GATEWAY_INTERNAL_HOST:$OBJECT_GATEWAY_INTERNAL_PORT\nexport AWS_ACCESS_KEY_ID=$STORAGE_ACCESS_KEY\nexport AWS_SECRET_ACCESS_KEY=$STORAGE_SECRET_KEY\n"})}),"\n",(0,o.jsx)(s.p,{children:"to access ceph externally,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"OBJECT_GATEWAY_EXTERNAL_HOST=$(kubectl -n rook-ceph get gw rook-ceph-rgw-rook-ceph -o json | jq -r '.spec.servers[0].hosts[0]')\nOBJECT_GATEWAY_EXTERNAL_PORT=31443\nexport AWS_HOST=$OBJECT_GATEWAY_EXTERNAL_HOST:$OBJECT_GATEWAY_EXTERNAL_PORT\nexport AWS_ENDPOINT=$OBJECT_GATEWAY_EXTERNAL_HOST:$OBJECT_GATEWAY_EXTERNAL_PORT\nexport AWS_ACCESS_KEY_ID=$STORAGE_ACCESS_KEY\nexport AWS_SECRET_ACCESS_KEY=$STORAGE_SECRET_KEY\n"})}),"\n",(0,o.jsx)(s.p,{children:"Sample commands to query ceph,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"s3cmd ls --host=${AWS_HOST} --no-check-certificate --no-ssl\ns3cmd mb --host=${AWS_HOST} --host-bucket=  s3://test-bucket --no-ssl\ns3cmd put data.csv --no-ssl --host=${AWS_HOST} --host-bucket=  s3://training-18107870-86eb-4c36-a40c-62bf77c9120c/4c56d172-6b52-409b-9308-75f1e8dd6418/9f3e2c75-782a-4735-85f0-461714cbcba0/dataset/data.csv\ns3cmd get s3://rookbucket/rookObj --recursive --no-ssl --host=${AWS_HOST} --host-bucket=  s3://rookbucket\ns3cmd ls s3://training-05d99a71-f0ca-4bfe-9a0c-8ebe09b0ddb8/ --host=${AWS_HOST} --host-bucket=  s3://training-05d99a71-f0ca-4bfe-9a0c-8ebe09b0ddb8/ --no-ssl\n"})}),"\n",(0,o.jsx)(s.h2,{id:"query-size-of-the-buckets",children:"Query Size of the Buckets"}),"\n",(0,o.jsxs)(s.ol,{children:["\n",(0,o.jsx)(s.li,{children:"log into ceph tools pod"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pods | grep rook-ceph-tools | cut -d ' ' -f1) bash\n"})}),"\n",(0,o.jsxs)(s.ol,{start:"2",children:["\n",(0,o.jsx)(s.li,{children:"Query bucket sizes"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"for bucket in $(radosgw-admin bucket list | jq -r '.[]' | xargs); do radosgw-admin bucket list --bucket=$bucket | jq -r '.[] | .name' | grep shadow | wc -l;done\n"})}),"\n",(0,o.jsx)(s.h2,{id:"query-size-of-buckets-using-s3cmd",children:"Query size of buckets using s3cmd"}),"\n",(0,o.jsxs)(s.p,{children:["Spin an s3cmd pod and export CEPH credentials by following steps under the section ",(0,o.jsx)(s.a,{href:"#tools-required-for-debugging",children:"ToolsRequiredForDebugging"})]}),"\n",(0,o.jsx)(s.p,{children:"Now we can query the size of buckets,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"s3cmd du -H s3://testbucket --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://testbucket\ns3cmd du -H s3://sf-logs --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://sf-logs\ns3cmd du -H s3://train-data --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://train-data\ns3cmd du -H s3://uipath --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://uipath\ns3cmd du -H s3://ml-model-files --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://ml-model-files\ns3cmd du -H s3://aifabric-staging --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://aifabric-staging\ns3cmd du -H s3://support-bundles --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://support-bundles\ns3cmd du -H s3://taskmining --host=${AWS_HOST} --no-check-certificate --no-ssl --host-bucket=  s3://taskmining\n"})}),"\n",(0,o.jsx)(s.h2,{id:"query-no-of-objects-pending-garbage-collection",children:"Query No of Objects Pending garbage collection"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"for bucket in $(radosgw-admin bucket list | jq -r '.[]' | xargs); do radosgw-admin bucket list --bucket=$bucket | jq -r '.[] | .name' | grep shadow | wc -l;done\n"})}),"\n",(0,o.jsxs)(s.p,{children:[(0,o.jsx)(s.strong,{children:"Note:"})," shadow object are those objects that have been deleted, but not garbage collected"]}),"\n",(0,o.jsx)(s.h2,{id:"troubleshooting-ceph",children:"Troubleshooting ceph"}),"\n",(0,o.jsx)(s.p,{children:"Ceph objecstore may become unavailable for a variety of reason, some of them are mentioned below"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"OSDs are full"}),"\n",(0,o.jsx)(s.li,{children:"Majority of OSDs are corrupted"}),"\n",(0,o.jsx)(s.li,{children:"OSD pods are not in healthy state due initCrashloop"}),"\n",(0,o.jsx)(s.li,{children:"Underlying block device is not available (either system level block device or LH provided block device)"}),"\n",(0,o.jsxs)(s.li,{children:["PGs are stuck in","\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Incomplete"}),"\n",(0,o.jsx)(s.li,{children:"Unfound"}),"\n",(0,o.jsx)(s.li,{children:"Pending"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.p,{children:"As a general thumb rule start with ceph status,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\n"})}),"\n",(0,o.jsx)(s.p,{children:"If it shows something like below , cluster is healthy"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"  cluster:\n    id:     0f9036e6-5ae0-4ff3-bdb1-853331280320\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 2h)\n    mgr: a(active, since 2h), standbys: b\n    osd: 3 osds: 3 up (since 2h), 3 in (since 2h)\n    rgw: 2 daemons active (rook.ceph.a, rook.ceph.b)\n\n  task status:\n\n  data:\n    pools:   8 pools, 81 pgs\n    objects: 2.48k objects, 876 MiB\n    usage:   5.6 GiB used, 762 GiB / 768 GiB avail\n    pgs:     81 active+clean\n\n  io:\n    client:   252 KiB/s rd, 426 B/s wr, 251 op/s rd, 168 op/s wr\n"})}),"\n",(0,o.jsx)(s.p,{children:"To understand specific warning or error,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph health detail\n"})}),"\n",(0,o.jsxs)(s.p,{children:["If OSD are reporting full, check OSD capacity and storage space consumed by actual objects in cluster\n",(0,o.jsx)(s.img,{alt:"CephUsage",src:n(101).Z+"",width:"1722",height:"316"})]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd df\n"})}),"\n",(0,o.jsx)(s.p,{children:"Sometime for a delete heavy cluster , you may see huge different between the two values while OSDs may report full. This is classic case of slower GC where deleted objects are still occupying storage space"}),"\n",(0,o.jsx)(s.p,{children:"In case OSD is full , we have two options"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Increase storage capacity of the cluster"}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"By adding new OSD (Raw device backed OSDs)"}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"By vertically scaling exiting OSD (PV backed OSDs)"}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Run GC manually. But to allow GC to cleanup the deleted objects , ceph cluster has to available for writes. When OSD become full, cluster becomes read-only. So to make cluster writable again , one can follow steps"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Get current full ratio"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd dump | grep ratio\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Increase full ratio by 0.01"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd set-full-ratio <NEW_VALUE>\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Run GC manually"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- radosgw-admin gc process --include-all\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Check ceph osd capacity to see if deleted objects are getting removed"}),"\n",(0,o.jsx)(s.li,{children:"Reset the full ratio to original value"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.h2,{id:"osds-are-crashing-due-to-init-error",children:"OSD(s) are crashing due to Init error"}),"\n",(0,o.jsx)(s.p,{children:"Ceph OSDs pod consists of Init containers and a main container. The job of init containers is to make sure everything is in place before main container starts. In this case, we see two init containers"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"active (Make sure ceph lvm devices are active and ready to use)"}),"\n",(0,o.jsx)(s.li,{children:"chown (Make sure data directory has correct permissions so that main container can access them using ceph user)"}),"\n"]}),"\n",(0,o.jsxs)(s.p,{children:[(0,o.jsx)(s.img,{alt:"OSDInitContainers",src:n(4487).Z+"",width:"1744",height:"288"}),"\n",(0,o.jsx)(s.img,{alt:"OSDInitError",src:n(6240).Z+"",width:"1232",height:"676"})]}),"\n",(0,o.jsx)(s.p,{children:"In such  case , one needs to check logs of crashing init container. To do that , run  below command"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph describe pod rook-ceph-osd-0-557c9b5f4b-7zktb\n"})}),"\n",(0,o.jsxs)(s.p,{children:[(0,o.jsx)(s.img,{alt:"OSDDescribePod1",src:n(2060).Z+"",width:"2438",height:"444"}),"\n",(0,o.jsx)(s.img,{alt:"OSDDescribePod2",src:n(3356).Z+"",width:"956",height:"530"})]}),"\n",(0,o.jsx)(s.p,{children:"It will show which container is having the problem (in this case it is activate) and why it is in error/crashloop state (in this case it is exiting with code 1)."}),"\n",(0,o.jsx)(s.p,{children:"Now view the logs of crashing container run below command,"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:" kubectl -n rook-ceph logs rook-ceph-osd-0-66d487bd96-zrq2m activate\n"})}),"\n",(0,o.jsx)(s.p,{children:(0,o.jsx)(s.img,{alt:"OSDDescribePodLogs",src:n(783).Z+"",width:"3084",height:"1751"})}),"\n",(0,o.jsx)(s.h2,{id:"pgs-stuck-in-incomplete-state",children:"PGs stuck in Incomplete state"}),"\n",(0,o.jsx)(s.p,{children:"Ceph PGs may get stuck in incomplete state and is not able to recover those PGs automatically. In such case, a manual intervention is required which would mark those PGs complete to make the objectstore usable"}),"\n",(0,o.jsx)(s.p,{children:"Below are steps to help achieve this"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Identify if PGs are stuck in incomplete state"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\n"})}),"\n",(0,o.jsxs)(s.p,{children:["Look for pgs section under ",(0,o.jsx)(s.code,{children:"data"})," if some PGs are showing Incomplete state , it requires manual intervention to recover"]}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Find PGs ids stuck in incomplete state"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph pg dump | grep complete  -i\n# output may look like below , first column is pg id\n\n8.10         190                   0         0          0        0  39739825            0           0   314       314  incomplete  2023-02-01T12:37:53.280816+0000     212'314      212:731  [0,1,2]           0  [0,1,2]               0         0'0  2023-01-31T18:34:53.950866+0000              0'0  2023-01-31T18:34:53.950866+0000              0\n8.11         155                   0         0          0        0  20483141            0           0   283       283  incomplete  2023-02-01T12:37:53.366529+0000     212'283      212:686  [0,2,1]           0  [0,2,1]               0         0'0  2023-01-31T18:34:53.950866+0000              0'0  2023-01-31T18:34:53.950866+0000              0\n8.12         171                   0         0          0        0  33379955            0           0   321       321  incomplete  2023-02-01T12:37:53.297156+0000     212'321      212:825  [1,2,0]           1  [1,2,0]               1         0'0  2023-01-31T18:34:53.950866+0000              0'0  2023-01-31T18:34:53.950866+0000              0\n8.13         152                   0         0          0        0  20308913            0           0   270       270  incomplete  2023-02-01T12:37:53.360517+0000     212'270      212:720  [2,1,0]           2  [2,1,0]               2\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Disable self heal for rook operator and scale it down. Make sure no operator pod is running"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Find active primary (column after array for ACTING, in this case osd 0,1,2) for all the affected PGs"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["\n",(0,o.jsx)(s.p,{children:"Edit one primary OSD at a time (Lets start with OSD 0)"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Before editing the OSD deployment , take backup of the live manifest"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph get deploy rook-ceph-osd-0 -o yaml > rook-ceph-osd-0.yaml\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["Edit OSD deployment to remove Probes and replace exiting command with a dummy command like ",(0,o.jsx)(s.code,{children:"sleep infinity"})," and wait for the pod to go into running state."]}),"\n",(0,o.jsx)(s.li,{children:"Mark those PGs complete one by one"}),"\n"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-osd-0 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0 --pgid <PG_ID> --op mark-complete\ne.g\nkubectl -n rook-ceph exec deploy/rook-ceph-osd-0 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0 --pgid 8.10 --op mark-complete\n\n# if you get below error\n# Mount failed with '(11) Resource temporarily unavailable'\n# it means the main OSD process is still running, make sure you edit OSD deployment \n# correctly, so that the OSD resouece is available to run adhoc command\n"})}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Once all the incomplete PGs are marked complete , reapply backed up manifest to revert\nthe temporary changes"}),"\n",(0,o.jsx)(s.li,{children:"Wait until the OSD join the cluster back"}),"\n",(0,o.jsx)(s.li,{children:"Repeat the same for other PGs"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.h2,{id:"pgs-stuck-in-recovery_unfound-state",children:"PGs stuck in recovery_unfound state"}),"\n",(0,o.jsx)(s.p,{children:"Ceph PGs may get stuck in recovery_unfound state. This may lead ceph cluster to ERR state. To recover from such state follow steps mentioned below"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["Find PGs stuck in recovery_unfound state","\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph pg dump | grep -i 'recovery_unfound'\n"})}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["Find objects in unfound state","\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph pg <PG_ID> list_unfound\n"})}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["Run revert to last known state","\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph pg <PG_ID> mark_unfound_lost revert\n"})}),"\n"]}),"\n",(0,o.jsxs)(s.li,{children:["Wait for those PGs to come out of ",(0,o.jsx)(s.code,{children:"recovery_unfound"})," state"]}),"\n"]}),"\n",(0,o.jsx)(s.h2,{id:"clock-skew-in-ceph-cluster",children:"Clock skew in ceph cluster"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Use chronyd with external ntp server (Preferred but may not work for airgap/offline env)"}),"\n",(0,o.jsxs)(s.li,{children:["Use chronyd with first server as ntp server (Require ",(0,o.jsx)(s.code,{children:"allow"})," directive in chrony config on to allow incoming sync request from other nodes)","\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"On other nodes, use first servers IPs/hostname as NTP server"}),"\n",(0,o.jsx)(s.li,{children:"Ensure UDP port designed for NTP (default: 123 ) is open among the nodes"}),"\n",(0,o.jsxs)(s.li,{children:["Use ",(0,o.jsx)(s.code,{children:"timedatectl"})," to check if time is getting synced"]}),"\n",(0,o.jsxs)(s.li,{children:["use ",(0,o.jsx)(s.code,{children:"chronyc sources"})," to check the ntp server being used by chrony service"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(s.p,{children:(0,o.jsx)(s.img,{alt:"CephClockSkew",src:n(3599).Z+"",width:"1330",height:"1158"})}),"\n",(0,o.jsx)(s.p,{children:"To fix clock skew , set clock to central server"}),"\n",(0,o.jsx)(s.p,{children:"Before fix,"}),"\n",(0,o.jsx)(s.p,{children:(0,o.jsx)(s.img,{alt:"CephClockBeforefix",src:n(9094).Z+"",width:"780",height:"646"})}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-bash",children:"date --set=\"$(ssh <SSH_USER>@<CENTRAL_SERVER_IP> 'date -u --rfc-3339=ns')\"\n"})}),"\n",(0,o.jsx)(s.p,{children:(0,o.jsx)(s.img,{alt:"CephClockAfterfix",src:n(7188).Z+"",width:"1504",height:"1954"})})]})}function h(e={}){const{wrapper:s}={...(0,c.a)(),...e.components};return s?(0,o.jsx)(s,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},101:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/ceph-usage-b571c9d377a84fb2f156a00c3dfe2b38.png"},7188:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/clock-skew-after-fix-32da6cc5c82a7a73e1b76f12448d97b1.png"},9094:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/clock-skew-before-fix-7a51158c1b09913d9b9aa7b48ba5ec16.png"},3599:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/clock-skew-ceph-e58b77c286a86fb51870fb3be5c781df.png"},3356:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/osd-describe-2-821ca59b5745303670f3a8f6d614e030.png"},2060:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/osd-describe-c9485bd03ed4749debeb83429c7da683.png"},4487:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/osd-init-containers-db52262f6a014d705391ce88a63ace4d.png"},6240:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/osd-init-error-1fa2237fdc9adc19265fa788a73561ec.png"},783:(e,s,n)=>{n.d(s,{Z:()=>o});const o=n.p+"assets/images/osd-logs-5e3ec4d74bb8c1d83d198b8fb6642a18.png"},1151:(e,s,n)=>{n.d(s,{Z:()=>r,a:()=>i});var o=n(7294);const c={},t=o.createContext(c);function i(e){const s=o.useContext(t);return o.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(c):e.components||c:i(e.components),o.createElement(t.Provider,{value:s},e.children)}}}]);